{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from collections import Counter\n",
    "\n",
    "# Wikilabels\n",
    "df = pd.read_csv('../data/wiki/wiki_name_race.csv')\n",
    "\n",
    "# drop rows with NAN forename or surname\n",
    "df.dropna(subset=['name_first', 'name_last'], inplace=True)\n",
    "\n",
    "# replace NAN middle name with empty string\n",
    "df.name_middle = df.name_middle.fillna('')\n",
    "\n",
    "sdf = df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# remove chars not unicode alphabets, dash or space\\nremove_special_chars = lambda x: ''.join([c if c.isalpha() or c == '-' or c == ' ' else '?' for c in x])\\nsdf['name_last'] = sdf['name_last'].apply(remove_special_chars)\\nsdf['name_first'] = sdf['name_first'].apply(remove_special_chars)\\nsdf['name_middle'] = sdf['name_middle'].apply(remove_special_chars)\\n\\nprint('surname char counts:')\\nprint(Counter(sdf['name_last'].str.cat()))\\nprint('\\n')\\nprint('forename char counts:')\\nprint(Counter(sdf['name_first'].str.cat()))\\nprint('\\n')\\nprint('midname char counts:')\\nprint(Counter(sdf['name_middle'].str.cat()))\\nprint('\\n')\\n\\n# drop rows with bad chars\\nsdf = sdf[~sdf.name_last.str.contains('[?]')]\\nsdf = sdf[~sdf.name_first.str.contains('[?]')]\\nsdf = sdf[~sdf.name_middle.str.contains('[?]')]\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "# remove chars not unicode alphabets, dash or space\n",
    "remove_special_chars = lambda x: ''.join([c if c.isalpha() or c == '-' or c == ' ' else '?' for c in x])\n",
    "sdf['name_last'] = sdf['name_last'].apply(remove_special_chars)\n",
    "sdf['name_first'] = sdf['name_first'].apply(remove_special_chars)\n",
    "sdf['name_middle'] = sdf['name_middle'].apply(remove_special_chars)\n",
    "\n",
    "print('surname char counts:')\n",
    "print(Counter(sdf['name_last'].str.cat()))\n",
    "print('\\n')\n",
    "print('forename char counts:')\n",
    "print(Counter(sdf['name_first'].str.cat()))\n",
    "print('\\n')\n",
    "print('midname char counts:')\n",
    "print(Counter(sdf['name_middle'].str.cat()))\n",
    "print('\\n')\n",
    "\n",
    "# drop rows with bad chars\n",
    "sdf = sdf[~sdf.name_last.str.contains('[?]')]\n",
    "sdf = sdf[~sdf.name_first.str.contains('[?]')]\n",
    "sdf = sdf[~sdf.name_middle.str.contains('[?]')]\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count distribution of ethnicity by surname, forename and middle name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       name_last\n",
      "race                                            \n",
      "Asian,GreaterEastAsian,EastAsian            2739\n",
      "Asian,GreaterEastAsian,Japanese             4107\n",
      "Asian,IndianSubContinent                    3802\n",
      "GreaterAfrican,Africans                     3149\n",
      "GreaterAfrican,Muslim                       4537\n",
      "GreaterEuropean,British                    15943\n",
      "GreaterEuropean,EastEuropean                6576\n",
      "GreaterEuropean,Jewish                      6285\n",
      "GreaterEuropean,WestEuropean,French         9503\n",
      "GreaterEuropean,WestEuropean,Germanic       3313\n",
      "GreaterEuropean,WestEuropean,Hispanic       6229\n",
      "GreaterEuropean,WestEuropean,Italian        8630\n",
      "GreaterEuropean,WestEuropean,Nordic         3168\n",
      "77981\n",
      "                                       name_first\n",
      "race                                             \n",
      "Asian,GreaterEastAsian,EastAsian             1813\n",
      "Asian,GreaterEastAsian,Japanese              3265\n",
      "Asian,IndianSubContinent                     4191\n",
      "GreaterAfrican,Africans                      2380\n",
      "GreaterAfrican,Muslim                        3026\n",
      "GreaterEuropean,British                      5326\n",
      "GreaterEuropean,EastEuropean                 2115\n",
      "GreaterEuropean,Jewish                       2866\n",
      "GreaterEuropean,WestEuropean,French          2477\n",
      "GreaterEuropean,WestEuropean,Germanic        1281\n",
      "GreaterEuropean,WestEuropean,Hispanic        2888\n",
      "GreaterEuropean,WestEuropean,Italian         2872\n",
      "GreaterEuropean,WestEuropean,Nordic          1394\n",
      "35894\n",
      "                                       name_middle\n",
      "race                                              \n",
      "Asian,GreaterEastAsian,EastAsian               540\n",
      "Asian,GreaterEastAsian,Japanese                258\n",
      "Asian,IndianSubContinent                      1053\n",
      "GreaterAfrican,Africans                        433\n",
      "GreaterAfrican,Muslim                          986\n",
      "GreaterEuropean,British                       2338\n",
      "GreaterEuropean,EastEuropean                   472\n",
      "GreaterEuropean,Jewish                         593\n",
      "GreaterEuropean,WestEuropean,French           1015\n",
      "GreaterEuropean,WestEuropean,Germanic          332\n",
      "GreaterEuropean,WestEuropean,Hispanic         1505\n",
      "GreaterEuropean,WestEuropean,Italian           809\n",
      "GreaterEuropean,WestEuropean,Nordic            655\n",
      "10989\n",
      "\n",
      "total rows = 133872, total classes = 13\n"
     ]
    }
   ],
   "source": [
    "# count surname freq\n",
    "surname_counts = sdf.groupby('race').agg({'name_last': 'nunique'})\n",
    "print(surname_counts)\n",
    "print(surname_counts.name_last.sum())\n",
    "\n",
    "# count forename freq\n",
    "forename_counts = sdf.groupby('race').agg({'name_first': 'nunique'})\n",
    "print(forename_counts)\n",
    "print(forename_counts.name_first.sum())\n",
    "\n",
    "# count middle name freq\n",
    "midname_counts = sdf.groupby('race').agg({'name_middle': 'nunique'})\n",
    "print(midname_counts)\n",
    "print(midname_counts.name_middle.sum())\n",
    "\n",
    "total_rows = sdf.shape[0]\n",
    "total_classes = surname_counts.shape[0]\n",
    "print('\\ntotal rows = %d, total classes = %d' % (total_rows, total_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize surname, forename and middle name into n-grams and apply TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total features = 133872\n"
     ]
    }
   ],
   "source": [
    "# concat last name, first name and middle name\n",
    "sdf['name'] = '_' + sdf['name_last'] + '_' + sdf['name_first'] + '_' + sdf['name_middle'] + '_'\n",
    "\n",
    "# build n-grams\n",
    "min_ngrams = 2\n",
    "max_ngrams = 4\n",
    "#vect = CountVectorizer(analyzer='char', max_df=0.3, min_df=3, ngram_range=(min_ngrams, max_ngrams), lowercase=True)\n",
    "vect = TfidfVectorizer(analyzer='char', ngram_range=(min_ngrams, max_ngrams), lowercase=True)\n",
    "X = vect.fit_transform(sdf.name)\n",
    "y = np.array(sdf.race.astype('category').cat.codes)\n",
    "\n",
    "# spit training and test datasets\n",
    "print('total features = %d' % X.shape[0])\n",
    "X_train,  X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 4398), (1, 5866), (2, 6289), (3, 2938), (4, 4994), (5, 33156), (6, 6663), (7, 8191), (8, 9834), (9, 3095), (10, 8330), (11, 9493), (12, 3850)]\n",
      "[(0, 4398), (1, 5866), (2, 6289), (3, 2938), (4, 4994), (5, 33156), (6, 6663), (7, 8191), (8, 9834), (9, 3095), (10, 8330), (11, 9493), (12, 3850)]\n"
     ]
    }
   ],
   "source": [
    "#from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "\n",
    "X_train_resampled = X_train\n",
    "y_train_resampled = y_train\n",
    "\n",
    "#X_train_sampled, y_train_resampled = RandomOverSampler(random_state=0).fit_sample(X_train, y_train)\n",
    "#X_train_sampled, y_train_resampled = SMOTE().fit_sample(X_train, y_train)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "print(sorted(Counter(y_train).items()))\n",
    "print(sorted(Counter(y_train_resampled).items()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Predict...\n",
      "Test score = 0.756004\n",
      "                                       precision    recall  f1-score   support\n",
      "\n",
      "     Asian,GreaterEastAsian,EastAsian       0.92      0.75      0.82      1099\n",
      "      Asian,GreaterEastAsian,Japanese       0.93      0.90      0.92      1467\n",
      "             Asian,IndianSubContinent       0.84      0.80      0.82      1572\n",
      "              GreaterAfrican,Africans       0.69      0.40      0.51       734\n",
      "                GreaterAfrican,Muslim       0.73      0.72      0.73      1248\n",
      "              GreaterEuropean,British       0.73      0.92      0.82      8289\n",
      "         GreaterEuropean,EastEuropean       0.81      0.77      0.79      1666\n",
      "               GreaterEuropean,Jewish       0.57      0.45      0.50      2048\n",
      "  GreaterEuropean,WestEuropean,French       0.71      0.64      0.67      2459\n",
      "GreaterEuropean,WestEuropean,Germanic       0.61      0.41      0.49       774\n",
      "GreaterEuropean,WestEuropean,Hispanic       0.76      0.72      0.74      2082\n",
      " GreaterEuropean,WestEuropean,Italian       0.78      0.76      0.77      2374\n",
      "  GreaterEuropean,WestEuropean,Nordic       0.85      0.65      0.73       963\n",
      "\n",
      "                             accuracy                           0.76     26775\n",
      "                            macro avg       0.76      0.68      0.72     26775\n",
      "                         weighted avg       0.75      0.76      0.75     26775\n",
      "\n",
      "[[ 822   39    7    3    9  175    5    9   10    2   11    6    1]\n",
      " [   9 1326    5    7    1   58    6    6    4    0   36    9    0]\n",
      " [   3    7 1251    9  103  126    6   12   19    3   15   15    3]\n",
      " [   4   16   33  293   79  171    6   23   54    4   26   23    2]\n",
      " [   2    2   68   23  903  111   33   54   23    2    8   15    4]\n",
      " [  29    7   44   27   40 7617   34  177  140   26   47   76   25]\n",
      " [   3    4   10    1   13  119 1286  120   30   29   13   29    9]\n",
      " [   4    6   27   10   40  689  106  919   81   49   65   41   11]\n",
      " [   7    3   12   23   27  492   23   84 1563   21   96  102    6]\n",
      " [   2    1    8    1    4  211   15   83   46  316   25   30   32]\n",
      " [   5    9    9   14    7  203   15   45   85   13 1508  159   10]\n",
      " [   4    4    7   10    4  210   28   43  105   18  118 1815    8]\n",
      " [   3    2    6    4    8  183   17   27   28   33   15   14  623]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "print('Train...')\n",
    "#model = XGBClassifier(max_depth=9, objective='multi:softmax', num_classes_total_classes)\n",
    "model = LogisticRegression(max_iter=300, solver='sag', class_weight=None, n_jobs=3)\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "print('Predict...')\n",
    "y_pred = model.predict(X_test)\n",
    "score = model.score(X_test, y_test)\n",
    "print('Test score = %f' % score)\n",
    "\n",
    "target_names = list(sdf.race.astype('category').cat.categories)\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  0  7  6  3  2 11  5]\n",
      "['Asian,GreaterEastAsian,Japanese', 'Asian,GreaterEastAsian,EastAsian', 'GreaterEuropean,Jewish', 'GreaterEuropean,EastEuropean', 'GreaterAfrican,Africans', 'Asian,IndianSubContinent', 'GreaterEuropean,WestEuropean,Italian', 'GreaterEuropean,British']\n"
     ]
    }
   ],
   "source": [
    "test_names = ['_abe_shinzo__', '_xi_jinping__', '_netanyahu_benjamin__', '_putin_vladimir__',\n",
    "              '_obama_barrack__', '_modi_narendra__', '_conte_giuseppe__', '_johnson_boris__']\n",
    "\n",
    "X_query = vect.transform(test_names).toarray()\n",
    "y_query = model.predict(X_query)\n",
    "\n",
    "print(y_query)\n",
    "print([target_names[i] for i in y_query])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from joblib import dump, load\n",
    "\n",
    "tfidf_filename = '../models/wiki/logistic_regression/tfidf.pkl'\n",
    "model_filename = '../models/wiki/logistic_regression/wiki_name_logreg.joblib'\n",
    "\n",
    "dump(model, model_filename)\n",
    "pickle.dump(vect, open(tfidf_filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "loaded_model = load(model_filename)\n",
    "\n",
    "# load vectorizer\n",
    "tf1 = pickle.load(open(tfidf_filename, 'rb'))\n",
    "loaded_vect = TfidfVectorizer(vocabulary=tf1.vocabulary_, analyzer='char', ngram_range=(min_ngrams, max_ngrams), lowercase=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
